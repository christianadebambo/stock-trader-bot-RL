{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF1_iYhIaMnJ"
   },
   "source": [
    "# Stock Trading Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1W7a-coednOm"
   },
   "source": [
    "The goal is to build a stock trading bot using Reinforcement Learning (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZxL9bqDjBBy"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the code\n",
    "\n",
    "import numpy as np                    # Library for numerical operations\n",
    "import pandas as pd                   # Library for data manipulation and analysis\n",
    "import tensorflow as tf               # Library for deep learning\n",
    "from datetime import datetime         # Library for working with dates and times\n",
    "import itertools                      # Library for efficient looping and combining elements\n",
    "import argparse                       # Library for parsing command-line arguments\n",
    "import re                             # Library for regular expressions\n",
    "import os                             # Library for interacting with the operating system\n",
    "import pickle                         # Library for serializing and deserializing Python objects\n",
    "import matplotlib.pyplot as plt       # Library for creating visualizations\n",
    "from sklearn.preprocessing import StandardScaler  # Library for standardizing features\n",
    "\n",
    "%matplotlib inline\n",
    "# Magic command to display matplotlib plots inline in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiUfHyrEjBB1"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  df = pd.read_csv('stock_data.csv')\n",
    "  return df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjB6nrhheEzl"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHjD1PpkjBB3"
   },
   "outputs": [],
   "source": [
    "class MultiStockEnvironment:\n",
    "    \n",
    "    def __init__(self, data, initial_investment=20000):\n",
    "        \"\"\"\n",
    "        Initializes the multi-stock environment.\n",
    "\n",
    "        Args:\n",
    "            data (numpy.ndarray): Stock price history data.\n",
    "            initial_investment (float): Initial investment amount (default: 20000).\n",
    "        \"\"\"\n",
    "\n",
    "        self.stock_price_history = data\n",
    "        self.n_step, self.n_stock = self.stock_price_history.shape\n",
    "\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "\n",
    "        self.action_space = np.arange(3**self.n_stock)\n",
    "\n",
    "        # Generate all possible combinations of actions using itertools\n",
    "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
    "\n",
    "        self.state_dim = self.n_stock * 2 + 1\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Initial observation/state.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cur_step = 0\n",
    "        self.stock_owned = np.zeros(self.n_stock)\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes the given action and advances the environment by one step.\n",
    "\n",
    "        Args:\n",
    "            action (int): Action to be performed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the new observation, reward, done flag, and additional information.\n",
    "        \"\"\"\n",
    "\n",
    "        assert action in self.action_space\n",
    "\n",
    "        prev_val = self._get_val()\n",
    "\n",
    "        self.cur_step = self.cur_step + 1\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "\n",
    "        self._trade(action)\n",
    "\n",
    "        cur_val = self._get_val()\n",
    "\n",
    "        reward = cur_val - prev_val\n",
    "\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "\n",
    "        info = {'cur_val': cur_val}\n",
    "\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation/state.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Current observation/state.\n",
    "        \"\"\"\n",
    "\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_stock] = self.stock_owned\n",
    "        obs[self.n_stock:2 * self.n_stock] = self.stock_price\n",
    "        obs[-1] = self.cash_in_hand\n",
    "        return obs\n",
    "\n",
    "    def _get_val(self):\n",
    "        \"\"\"\n",
    "        Calculates the current portfolio value.\n",
    "\n",
    "        Returns:\n",
    "            float: Current portfolio value.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
    "\n",
    "    def _trade(self, action):\n",
    "        \"\"\"\n",
    "        Performs the trading based on the given action.\n",
    "\n",
    "        Args:\n",
    "            action (int): Action to be performed.\n",
    "        \"\"\"\n",
    "\n",
    "        action_vector = self.action_list[action]\n",
    "\n",
    "        sell_index = []\n",
    "        buy_index = []\n",
    "        for i, a in enumerate(action_vector):\n",
    "            if a == 0:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                buy_index.append(i)\n",
    "\n",
    "        if sell_index:\n",
    "            # Sell stocks\n",
    "            for i in sell_index:\n",
    "                self.cash_in_hand = self.cash_in_hand + self.stock_price[i] * self.stock_owned[i]\n",
    "                self.stock_owned[i] = 0\n",
    "\n",
    "        if buy_index:\n",
    "            # Buy stocks\n",
    "            buy_me = True\n",
    "            while buy_me:\n",
    "                for i in buy_index:\n",
    "                    if self.cash_in_hand > self.stock_price[i]:\n",
    "                        self.stock_owned[i] = self.stock_owned[i] + 1\n",
    "                        self.cash_in_hand = self.cash_in_hand - self.stock_price[i]\n",
    "                    else:\n",
    "                        # Stop buying if not enough cash\n",
    "                        buy_me = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxSpFCRXgwS1"
   },
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UqcgvFbjBB6"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A replay buffer for storing and sampling transitions for reinforcement learning.\n",
    "\n",
    "    Args:\n",
    "        obs_dim (int): Dimension of the observation space.\n",
    "        act_dim (int): Dimension of the action space.\n",
    "        size (int): Maximum size of the replay buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        \"\"\"\n",
    "        Initializes the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            obs_dim (int): Dimension of the observation space.\n",
    "            act_dim (int): Dimension of the action space.\n",
    "            size (int): Maximum size of the replay buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)   # Buffer for storing current observations\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)   # Buffer for storing next observations\n",
    "        self.acts_buf = np.zeros(size, dtype=np.uint8)                # Buffer for storing actions\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)              # Buffer for storing rewards\n",
    "        self.done_buf = np.zeros(size, dtype=np.uint8)                # Buffer for storing done flags\n",
    "        self.ptr = 0                                                   # Pointer for the next available index\n",
    "        self.size = 0                                                  # Current size of the buffer\n",
    "        self.max_size = size                                           # Maximum size of the buffer\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        \"\"\"\n",
    "        Stores a transition in the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            obs (numpy.ndarray): Current observation.\n",
    "            act (int): Action taken.\n",
    "            rew (float): Reward received.\n",
    "            next_obs (numpy.ndarray): Next observation.\n",
    "            done (bool): Done flag indicating if the episode terminated.\n",
    "        \"\"\"\n",
    "\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size    # Update the pointer by wrapping around the buffer\n",
    "        self.size = min(self.size + 1, self.max_size) # Increment the size of the buffer while ensuring it doesn't exceed the maximum size\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Samples a batch of transitions from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of transitions to sample (default: 32).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the sampled batch of transitions with keys 's1', 's2', 'a', 'r', 'd'.\n",
    "                  's1' and 's2' represent the current and next observations respectively.\n",
    "                  'a' represents the actions taken.\n",
    "                  'r' represents the rewards received.\n",
    "                  'd' represents the done flags indicating if the episodes terminated.\n",
    "        \"\"\"\n",
    "\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)  # Randomly sample indices from the buffer\n",
    "        return dict(s1=self.obs1_buf[idxs], s2=self.obs2_buf[idxs], a=self.acts_buf[idxs], r=self.rews_buf[idxs], d=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghodEtQ0xGLq"
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "301ym_OZjBCE"
   },
   "outputs": [],
   "source": [
    "def action_NN(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):\n",
    "    \"\"\"\n",
    "    Creates a neural network model for action selection.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input.\n",
    "        n_action (int): Number of possible actions.\n",
    "        n_hidden_layers (int): Number of hidden layers (default: 1).\n",
    "        hidden_dim (int): Dimension of the hidden layers (default: 32).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.models.Model: Action selection neural network model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input layer\n",
    "    i = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    x = i\n",
    "\n",
    "    # Add the specified number of hidden layers\n",
    "    for layer in range(n_hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_dim, activation='relu')(x)\n",
    "\n",
    "    # Output layer for action selection\n",
    "    x = tf.keras.layers.Dense(n_action)(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.models.Model(inputs=i, outputs=x)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer=tf.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False))\n",
    "\n",
    "    \n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDI0DWP1mG3t"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg33On1ejBCG"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    \"\"\"\n",
    "    Deep Q-Network agent for reinforcement learning.\n",
    "\n",
    "    Args:\n",
    "        state_size (int): Dimension of the state space.\n",
    "        action_size (int): Dimension of the action space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Initializes the DQNAgent.\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of the state space.\n",
    "            action_size (int): Dimension of the action space.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "        self.gamma = 0.95                                                            # Discount factor for future rewards\n",
    "        self.epsilon = 1.0                                                           # Exploration rate\n",
    "        self.epsilon_min = 0.01                                                      # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.995                                                    # Decay rate for exploration rate\n",
    "        self.model = action_NN(state_size, action_size)                                # Neural network model for action selection\n",
    "\n",
    "    def update_replay_buffer(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Updates the replay buffer with a new transition.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state.\n",
    "            action (int): Action taken.\n",
    "            reward (float): Reward received.\n",
    "            next_state (numpy.ndarray): Next state.\n",
    "            done (bool): Done flag indicating if the episode terminated.\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state.\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)  # Exploration: select a random action\n",
    "\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # Exploitation: select the action with the highest Q-value\n",
    "\n",
    "    def replay(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Performs replay to update the agent's Q-network.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Number of transitions to sample from the replay buffer (default: 32).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.memory.size < batch_size:\n",
    "            return  # Not enough samples in the replay buffer, skip replay\n",
    "\n",
    "        minibatch = self.memory.sample_batch(batch_size)\n",
    "        states = minibatch['s1']\n",
    "        next_states = minibatch['s2']\n",
    "        actions = minibatch['a']\n",
    "        rewards = minibatch['r']\n",
    "        done = minibatch['d']\n",
    "\n",
    "        target = rewards + self.gamma * np.max(self.model.predict(next_states), axis=1)\n",
    "\n",
    "        target[done] = rewards[done]  # Set the target to be the immediate reward if the episode terminated\n",
    "\n",
    "        target_full = self.model.predict(states)\n",
    "        target_full[np.arange(batch_size), actions] = target\n",
    "\n",
    "        self.model.train_on_batch(states, target_full)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay  # Decay the exploration rate\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Loads the model weights from a file.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the file to load the weights from.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Saves the model weights to a file.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the file to save the weights to.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xSwX9QgjBB_"
   },
   "outputs": [],
   "source": [
    "def make_dir(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtZtSHtwjBCB"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(choice):\n",
    "  a = np.load(f'{rewards_folder}/{choice}.npy')\n",
    "\n",
    "  print(f\"Average Reward: {a.mean():.2f} | Min: {a.min():.2f} | Max: {a.max():.2f}\")\n",
    "\n",
    "  plt.hist(a, bins=30)\n",
    "  plt.title(choice)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNPtLZsVswez"
   },
   "source": [
    "## Standardizing / Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWfjGs-sjBB9"
   },
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    \"\"\"\n",
    "    Generates a scaler for normalizing state values.\n",
    "\n",
    "    Args:\n",
    "        env (MultiStockEnvironment): Environment object.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.preprocessing.StandardScaler: Scaler for normalizing state values.\n",
    "    \"\"\"\n",
    "\n",
    "    states = []\n",
    "    for _ in range(env.n_step):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFmfrgA6LqUJ"
   },
   "source": [
    "## Perform and Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ww19OVexjBCI"
   },
   "outputs": [],
   "source": [
    "def play_one_episode(agent, env, is_train):\n",
    "    \"\"\"\n",
    "    Plays one episode of the environment using the specified agent.\n",
    "\n",
    "    Args:\n",
    "        agent (DQNAgent): DQN agent.\n",
    "        env (MultiStockEnvironment): Environment object.\n",
    "        is_train (str): Training mode flag ('train' or 'test').\n",
    "\n",
    "    Returns:\n",
    "        float: The final portfolio value achieved in the episode.\n",
    "    \"\"\"\n",
    "\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])  # Normalize the state using the scaler\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)  # Select an action\n",
    "        next_state, reward, done, info = env.step(action)  # Take a step in the environment\n",
    "        next_state = scaler.transform([next_state])  # Normalize the next state using the scaler\n",
    "\n",
    "        if is_train == 'train':\n",
    "            agent.update_replay_buffer(state, action, reward, next_state, done)  # Update the replay buffer\n",
    "            agent.replay(batch_size)  # Perform replay to update the agent's Q-network\n",
    "\n",
    "        state = next_state  # Update the current state\n",
    "\n",
    "    return info['cur_val']  # Return the final portfolio value achieved in the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdlKE7DWjBCL"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bXfVfxd_jBCM",
    "outputId": "c3db5440-29fc-453a-c809-d317fef62086"
   },
   "outputs": [],
   "source": [
    "models_folder = '../root/models/'\n",
    "rewards_folder = '../root/rewards/'\n",
    "num_episodes = 500\n",
    "batch_size = 32\n",
    "initial_investment = 50000\n",
    "\n",
    "make_dir(models_folder)\n",
    "make_dir(rewards_folder)\n",
    "\n",
    "data = get_data()\n",
    "n_timesteps, n_stocks = data.shape\n",
    "\n",
    "choice = 'train'\n",
    "\n",
    "n_train = n_timesteps // 2\n",
    "\n",
    "train_data = data[:n_train]\n",
    "test_data = data[n_train:]\n",
    "\n",
    "env = MultiStockEnvironment(train_data, initial_investment)\n",
    "state_size = env.state_dim\n",
    "action_size = len(env.action_space)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scaler = get_scaler(env)\n",
    "\n",
    "portfolio_value = []\n",
    "\n",
    "if choice == 'test':\n",
    "    # Load scaler and environment for testing\n",
    "    with open(f'{models_folder}/standard_scaler.dat', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    env = MultiStockEnvironment(test_data, initial_investment)\n",
    "\n",
    "    # Set exploration rate to a small value for testing\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    # Load pre-trained agent model for testing\n",
    "    agent.load(f'{models_folder}/nn_agent.h5')\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, choice)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"Episode {e+1}/{num_episodes}\\nEpisode end value: {val:.2f}  |  Duration: {dt}\")\n",
    "    portfolio_value.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sinGawJdYWAK"
   },
   "outputs": [],
   "source": [
    "if choice == 'train':\n",
    "    # Save the trained agent model\n",
    "    agent.save(f'{models_folder}/nn_agent.h5')\n",
    "\n",
    "    # Save the scaler\n",
    "    with open(f'{models_folder}/standard_scaler.dat', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "# Save the portfolio values\n",
    "np.save(f'{rewards_folder}/{choice}.npy', portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "32qZ1hfoLSqV",
    "outputId": "40be80f3-5e03-4f99-d503-0ae09161f309"
   },
   "outputs": [],
   "source": [
    "plot_rewards(choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nH17cV2jjBCQ"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xdiQJNN2jBCR",
    "outputId": "b5aef38a-6d36-43c7-d731-78ed3474c2b2"
   },
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "batch_size = 32\n",
    "initial_investment = 30000\n",
    "\n",
    "choice = 'test'\n",
    "\n",
    "state_size = env.state_dim\n",
    "action_size = len(env.action_space)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "portfolio_value = []\n",
    "\n",
    "if choice == 'test':\n",
    "    # Load the saved scaler for testing\n",
    "    with open(f'{models_folder}/standard_scaler.dat', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    env = MultiStockEnvironment(test_data, initial_investment)\n",
    "\n",
    "    # Set exploration rate to a small value for testing\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    # Load pre-trained agent model for testing\n",
    "    agent.load(f'{models_folder}/nn_agent.h5')\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, choice)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"Episode {e+1}/{num_episodes}\\nEpisode end value: {val:.2f}  |  Duration: {dt}\")\n",
    "    portfolio_value.append(val)\n",
    "\n",
    "# Save the portfolio values\n",
    "np.save(f'{rewards_folder}/{choice}.npy', portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "WQ5BlBaHjBCT",
    "outputId": "e1d56fa6-e257-4b32-d5bf-65ac5b3ef894"
   },
   "outputs": [],
   "source": [
    "plot_rewards(choice)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "stock_trading_bot_RL.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
